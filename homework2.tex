\documentclass[a4paper,11pt,fleqn]{article}
\usepackage[utf8]{inputenc}
%\usepackage[italian]{babel}
%\usepackage{lipsum}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{multirow}
\usepackage{hyphenat}
\usepackage{sectsty}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{subfigure}
%\usepackage{color}
\usepackage{hyperref}


\title{Homework 2}
\author{Alessandro Bianchetti}

\begin{document}
\maketitle
\textit{Note: calculations of this work were cross-checked with the students Mila Racca and Kai Aidan Growcoot. 
This report is however independent and different from theirs.}

\section{Exercise 4}
The purpose here is to prove that the characteristic function of a MVN is given by
\begin{equation}
    \phi(\bf{k}) = \exp{(-i\mu^T\cdot \bf{k}-\frac{1}{2}\bf{k}^T C^{-1}\bf{k})}
\end{equation}
Let's first remind the definition of characteristic function:
\begin{equation}
    \label{eqn:char}
    \phi(\bf{k}) = \int d^nx \exp{(-i\bf{k}\cdot \bf{x} )p(\bf{x})}
\end{equation}
Also recall the form of the MVN function
\begin{equation}
    \mathcal{N}(\bf{x}|\bf{\mu}, C) = \frac{1}{(2\pi)^{N/2}\sqrt{\det{C}}} \exp{\left\{-\frac{1}{2}(\bf{x}-\bf{\mu})^T C^{-1} (\bf{x}-\bf{\mu})\right\}}
\end{equation}
So let's plug this definition inside \ref*{eqn:char} and see what happens. First,
we're going to apply the square completion technique, thus solving the Fourier Transform.
Then we're going to do it by rotating the covariance matrix C making it diagonal.

\subsection{Method 1: square completion}
For simplicity, from now on we refer to the normalization factor of the multivariate Gaussian as $A$.
\begin{gather*}
    \phi(\bf{k}) = \int d^nx \exp{(-i\bf{k}\cdot \bf{x}) A \exp{\left\{-\frac{1}{2}(\bf{x}-\bf{\mu})^T C^{-1} (\bf{x}-\bf{\mu})\right\}}} = \\
    = \int d^n u A \exp{\left\{ -\frac{1}{2}\bf{u}^TC^{-1}\bf{u} - i\bf{k}^T(\bf{u}+\bf{\mu}) \right\}}
\end{gather*}
where we replaced $\bf{u} = \bf{x}-\bf{\mu}$. Let's play on the exponent a bit in order to build a new squared
quantity at the exponent, this time involving the frequency $\bf{k}$. The formula of the square completion is given by
\begin{align}
    \label{eqn:square_completion}
    \bf{x}^T A \bf{x} + \bf{b}\cdot \bf{x} + c = (\bf{x}+\bf{m})^TA(\bf{x}+\bf{m})+\bf{n} \\
    \textnormal{where} \qquad \bf{m} = \frac{1}{2}A^{-1}\bf{b} \qquad \bf{n} = c -\frac{1}{4}\bf{b}^TA^{-1}\bf{b}
\end{align}
So in our case
\begin{multline*}
   -\frac{1}{2}\bf{u}^T C^{-1}\bf{u} - i\bf{k} \cdot (\bf{u}+\bf{\mu}) = -\frac{1}{2}\bf{u}^TC^{-1}\bf{u} - i \bf{k}\cdot \bf{u} -i \bf{k}\cdot \bf{\mu}
%    = -\frac{1}{2}\bf{u}^TC^{-1}\bf{u} - \frac{i}{2} \bf{k}\cdot \bf{u} - \frac{i}{2} \bf{k}\cdot \bf{u} -i \bf{k}\cdot \bf{\mu} = \\
%    = -\frac{1}{2}\bf{u}^TC^{-1}\bf{u} - \frac{i}{2} \bf{k}^T \bf{u} - \frac{i}{2} \bf{u}^T \bf{k} -i \bf{k}\cdot \bf{\mu} = \\
%    = -\frac{1}{2}\bf{u}^TC^{-1}\bf{u} - \frac{i}{2} \bf{k}^T C C^{-1} \bf{u} - \frac{i}{2} \bf{u}^T C^{-1}C \bf{k} -i \bf{k}\cdot \bf{\mu} = \\
%    = = -\frac{1}{2}\bf{u}^TC^{-1}\bf{u} - \frac{i}{2} \bf{k}^T C C^{-1} \bf{u} - \frac{i}{2} \bf{u}^T C^{-1}C \bf{k} +\frac{1}{2}\bf{k}^T C \bf{k} -\frac{1}{2}\bf{k}^T C \bf{k} -i \bf{k}\cdot \bf{\mu} = \\
%    = -\frac{1}{2}(\bf{u}+iC\bf{k})^T C^{-1} (\bf{u}+iC\bf{k}) -\frac{1}{2}\bf{k}^T C \bf{k} - i \bf{k} \cdot\bf{\mu}  
\end{multline*}
Therefore
\begin{equation*}
    \begin{cases}
        A = -\frac{1}{2}C^{-1} \\
        \bf{b} = -i\bf{k} \\
        c = -i\bf{k}\cdot \bf{\mu}
    \end{cases}
\end{equation*}
and the exponent will look like
\begin{multline*}
    -\frac{1}{2}\bf{u}^TC^{-1}\bf{u} - i \bf{k}\cdot \bf{u} -i \bf{k}\cdot \bf{\mu} = = -\frac{1}{2}\left[(\bf{u}+iC\bf{k})^T C^{-1} (\bf{u}+iC\bf{k})\right] -\frac{1}{2}\bf{k}^T C \bf{k} - i \bf{k} \cdot\bf{\mu}
\end{multline*}
We now apply this to the integral we were working on:
\begin{gather*}
    \phi(\bf{k}) = \exp{\left[-\frac{1}{2}\bf{k}^T C \bf{k} - i \bf{k}\cdot \bf{\mu}\right]} \int d^n u A \exp{\left[ -\frac{1}{2}(\bf{u}+iC\bf{k})^T C^{-1} (\bf{u}+iC\bf{k}) \right]}
\end{gather*}
and we can again change variable $\bf{t} = \bf{u}+iC\bf{k}$, so that
\begin{gather*}
    \phi(\bf{k}) = \exp{\left[-\frac{1}{2}\bf{k}^T C \bf{k} - i \bf{k}\cdot \bf{\mu}\right]} \int d^n t A \exp{\left[ -\frac{1}{2}\bf{t}^T C^{-1} \bf{t} \right]}
\end{gather*}
but in this last equation the integral is a perfectly normalised Gaussian integrated over infinity, meaning
it corresponds to 1. Finally:
\begin{gather*}
    \phi(\bf{k}) = \exp{\left[-\frac{1}{2}\bf{k}^T C \bf{k} - i \bf{k} \cdot \bf{\mu}\right]}
\end{gather*}


\paragraph{Alternative solution}
It is perhaps possible to try out a different path. In fact, after the first substitution (introducing $\bf{u}$), we can immediately
make the following change of variables, switching to $\bf{t}$:
\[
    \sqrt{C}\bf{t} = \bf{u}
\]
where $\sqrt{C}$ is defined as the squared root of the matrix C, meaning a matrix such that $\sqrt{C} \sqrt{C} = C$. This means that
\[
\bf{u}^T C \bf{u} = (\sqrt{C}\bf{t})^T C^{-1}\sqrt{C}\bf{t} = \bf{t} \cdot \bf{t} = t^2    
\]
Such a change of coordinates will also introduce the determinant of the transformation matrix $\sqrt{C}$ in the integral. Moreover,
$\det{\sqrt{C}}=\sqrt{\det{C}}$ due to Binet's Theorem. 
\begin{gather*}
    \phi(\bf{k})= \int d^n u A \exp{\left\{ -\frac{1}{2}\bf{u}^TC^{-1}\bf{u} - i\bf{k}^T(\bf{u}+\bf{\mu}) \right\}} = \\
    = A\sqrt{\det{C}}\int d^n t e^{-i\bf{k}\cdot \bf{\mu}}e^{-i\bf{k}\sqrt{C}\bf{t}}e^{-\frac{1}{2}t^2} = A\sqrt{\det{C}}e^{-i\bf{k}\cdot \bf{\mu}} \int d^n t e^{-\frac{1}{2}[t^2+2i\bf{k}\sqrt{C}\bf{t}]}
\end{gather*}
Here we perform a square completion by adding and subtracting the term $\bf{k}^TC\bf{k}$
\begin{gather*}
    \phi(\bf{k}) = A\sqrt{\det{C}} e^{-i\bf{k}\cdot \bf{\mu}}e^{-\frac{1}{2}\bf{k}^T C \bf{k}}\int d^nt e^{-\frac{1}{2}[\bf{t}+i\sqrt{C}\bf{k}]^2}
\end{gather*}
A final substitution $\bf{y}=\bf{t}+i\sqrt{C}\bf{k}$ leads to 
\begin{gather*}
    \phi(\bf{k}) = A\sqrt{\det{C}} e^{-i\bf{k}\cdot \bf{\mu}}e^{-\frac{1}{2}\bf{k}^T C \bf{k}}\int d^ny e^{-\frac{1}{2}y^2}
\end{gather*}
But the integral in the latter is equal to $(2\pi)^{N/2}$, and therefore it cancels out with the term
$A\sqrt{\det{C}}=(2\pi)^{-N/2}$. So we're left with 
\begin{gather*}
    \phi(\bf{k}) = e^{-i\bf{k}\cdot \bf{\mu}}e^{-\frac{1}{2}\bf{k}^T C \bf{k}}
\end{gather*}
which is again the expected result.

\subsection{Method 2: diagonalization}

This time we try introducing a rotation, governed by a rotation matrix.
\begin{equation}
    R(\theta) = 
    \begin{pmatrix}
        \cos{\theta} & \sin{\theta} \\
        \sin{\theta} & \cos{\theta} 
    \end{pmatrix}
\end{equation}
With a change of coordinates imposed by a rotation matrix there is no need
to worry about introducing the determinant of the Jacobian as we did in the
previous section, since $\det{R(\theta)}=1$ $\forall \theta$.
We choose R so to make the inverse of the covariance matrix diagonal $R^T C^{-1} R = \Lambda$.

So let's start again from the point where we've already made the shift of 
coordinates, introducing $\bf{u} = \bf{x}-\bf{\mu}$:
\begin{equation*}
    \phi(\bf{k})= \int d^n u A \exp{\left[ -\frac{1}{2}\bf{u}^TC^{-1}\bf{u} - i\bf{k}^T(\bf{u}+\bf{\mu}) \right]}
\end{equation*}
Now we set $\bf{u} = R\bf{y}$
\begin{gather*}
    \phi(\bf{k})= \int d^n y A \exp{\left[ -\frac{1}{2}\bf{y}^T R^T C^{-1} R\bf{y} - i\bf{k}^T(R\bf{y}+\bf{\mu}) \right]} = \\
    = \int d^n y A \exp{\left[ -\frac{1}{2}\bf{y}^T \Lambda \bf{y} - i\bf{k}^T R \bf{y} -i \bf{k}^T\bf{\mu} \right]}
\end{gather*}
Now we better define $\bf{z} = \bf{k}^T R$, so that 
\begin{gather*}
    \phi(\bf{k})= \int d^n y A \exp{\left[ \sum_i(\frac{1}{2}\lambda_iy_i^2-h_iy_i) -i \bf{k}^T\bf{\mu} \right]} \\
    = A \exp{-i\bf{k}\cdot \bf{\mu}} \int d^ny \Pi_i \exp{\left[-\frac{1}{2}\lambda_i y_i^2 -z_i y_i\right]}  
\end{gather*}
And we proceed again by completing the square:
\begin{gather*}
    \phi(\bf{k}) = A \exp{-i\bf{k}\cdot \bf{\mu}} \Pi_i \int d^ny \exp{\left[-\left(\sqrt{\frac{\lambda_i}{2}}y_i + \frac{z_i}{\sqrt{2\lambda_i}}\right)^2 \right]} \exp{\frac{z_i^2}{2\lambda_i}} =  \\
    = A \exp{\left[-i\bf{k}\cdot\bf{\mu} \right]} \Pi_i \sqrt{\frac{2}{\lambda_i}}(2\pi)^{1/2}\exp{\frac{h_i^2}{2\lambda_i}}
\end{gather*}
Remember $A=(2\pi)^{-N/2}(\det{C})^{-1/2}$, and $\det{C}^{-1/2}=\Pi_i\sqrt{\lambda_i}$ using eigenvalues.
\begin{gather*}
    \phi(\bf{k}) = (2\pi)^{-N/2} \Pi_i\sqrt{\lambda_i} \exp{-i\bf{k}\cdot \bf{\mu}} (2\pi)^{N/2} \Pi_i \sqrt{\lambda_i} \sqrt{\frac{2}{\lambda_i}}\exp{\frac{z_i^2}{2\lambda_i}} \\
    = \exp{\left[-i\bf{k}\cdot \bf{\mu}\right]} \Pi_i \exp{\left[\frac{z_i^2}{2\lambda_i}\right]} = \exp{\left[-i\bf{k}\cdot \bf{\mu}\right]} \exp{\left[-\frac{\bf{k}^TR\Lambda^{-1} R^T\bf{k}}{2}\right]}
\end{gather*}
but $R \Lambda^{-1} R^T = C$, so finally 
\begin{gather*}
    \phi(\bf{k}) = \exp{\left[-i\bf{k}\cdot\bf{\mu}-\frac{1}{2}\bf{k}^TC\bf{k}\right]}
\end{gather*}
which is the result we wanted to achieve.



%-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------%


\section{Exercise 5}

The characteristic function also has the property of generating momenta, if 
we derive on the generic components $-ik_{\alpha}$. We want to test such 
property 
\begin{equation}
    \label{eqn:generation}
    E[x_{\alpha}^{n_{\alpha}} \dotsm x_{\beta}^{n_{\beta}}] = \frac{\partial^{n_\alpha+\dots+n_{\beta}}\phi(\vec{k})}{\partial^{n_\alpha}(-ik_{\alpha})\dotsm \partial^{n_\beta}(-ik_{\beta})}\Big|_{\vec{k}=0}
\end{equation}
for two simple cases: the mean of a single component $x_{\alpha}$ and the 
covariance of two components $x_{\alpha}$ and $x_{\beta}$.

\subsection{Mean}
For the first step, we try to compute the derivative in the RHS of 
\ref{eqn:generation} for $n_{\alpha}=1$ and $n_{\gamma}=0$ 
$\forall \gamma \neq \alpha$. Therefore 
\begin{gather*}
    \label{eqn:der}
    \frac{\partial \phi(\vec{k})}{\partial(-ik_{\alpha})}\Big|_{\vec{k}=0} = i \frac{\partial \phi(\vec{k})}{\partial k_{\alpha}}\Big|_{\vec{k}=0} = i \left[T_{\alpha}\phi(\vec{k})\right]\Big|_{\vec{k}=0}
\end{gather*}
where $T_{\alpha}$ is the internal derivative, meaning the derivative of the 
exponent with respect to $k_{\alpha}$.
\begin{gather*}
    T_{\alpha} = \frac{\partial}{\partial k_{\alpha}}\left[-i\sum_i k_i \mu_i -\frac{1}{2}\sum_i \sum_j k_i C_{ij}k_j\right] = -i\mu_{\alpha} \sum_jC_{\alpha j}k_j
\end{gather*}
Now let's evaluate the expression at $\vec{k}=0$. Actually $\phi(\vec{k})|_{\vec{k}=0}=1$
and $T_{\alpha}|_{\vec{k}=0}=-i\mu_{\alpha}$. So \ref{eqn:der} becomes
\begin{gather*}
    \frac{\partial \phi(\vec{k})}{\partial(-ik_{\alpha})}\Big|_{\vec{k}=0} = i \cdot (-i\mu_{\alpha}) = \mu_{\alpha}
\end{gather*}
but that is exactly $E[x_{\alpha}]$, so we correctly generated a first-order 
momentum of expectation, proving the generating equation right.


\subsection{Covariance}
The model we have to follow is the same as before, but this time we have to 
perform a second derivative, over the coordinates $k_{\alpha}$ and $k_{\beta}$.
\begin{gather*}
    \frac{\partial^2\phi(\vec{k})}{(-i)^2\partial k_{\alpha}\partial k_{\beta}}\Big|_{\vec{k}=0} = -\frac{\partial \phi (\vec{k})}{\partial k_{\alpha}\partial k_{\beta}} \Big|_{\vec{k}=0}= -\frac{\partial}{\partial k_{\alpha}}\left[\frac{\partial \phi(\vec{k})}{\partial k_{\beta}}\right]\Big|_{\vec{k}=0} =\\
    = \frac{\partial}{\partial k_{\alpha}}\left[T_{\beta}\phi(\vec{k})\right]\Big|_{\vec{k}=0} = \left[-\frac{\partial T_{\beta}}{\partial k_{\alpha}} -T_{\alpha}T_{\beta}\phi(\vec{k})\right]\Big|_{\vec{k}=0}
\end{gather*}
where
\begin{gather*}
    \frac{\partial T_{\beta}}{\partial k_{\alpha}} = \frac{\partial}{\partial k_{\alpha}}\left[-i\mu_{\beta}-\sum_j C_{\beta j}k_j\right] = -C_{\alpha \beta}.
\end{gather*}
This means that
\begin{gather*}
    \frac{\partial^2\phi(\vec{k})}{(-i)^2\partial k_{\alpha}\partial k_{\beta}}\Big|_{\vec{k}=0} = [C_{\alpha \beta}-T_{\alpha}T_{\beta}]\phi(\vec{k})\Big|_{\vec{k}=0} = [C_{\alpha \beta}- (-i\mu_{\alpha})(-i\mu_{\beta})] = C_{\alpha \beta} +\mu_{\alpha}\mu_{\beta}
\end{gather*}
This is the result we wanted to reach. In fact 
\begin{equation}
    Cov[x_{\alpha}x_{\beta}] = E[x_{\alpha}x_{\beta}] - E[x_{\alpha}]E[x_{\beta}] = C_{\alpha \beta} +\mu_{\alpha}\mu_{\beta} - \mu_{\alpha}\mu_{\beta} = C_{\alpha \beta}
\end{equation}
which is the correct result. 






%-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------%


\section{Exercise 6}
The idea here is proving that the characteristic function we derived is again Gaussian-shaped.
So we will just integrate $\phi(\bf{k})$ over the frequency space and hope
we get a simple constant.
\begin{equation*}
    \int d^nk \exp{\left[-i\bf{k}\cdot\bf{\mu}-\frac{1}{2}\bf{k}^TC\bf{k}\right]}
\end{equation*}
Again we complete the square of the exponent by applying \ref{eqn:square_completion}.
This time 
\begin{gather*}
    \begin{cases}
        \bf{m} = -iC^{-1}\bf{\mu} \\
        n = \mu^TC^{-1}\mu
    \end{cases}
\end{gather*}
and the exponent becomes
\begin{gather*}
    -i\bf{k}\cdot\bf{\mu}-\frac{1}{2}\bf{k}^TC\bf{k} = (\bf{k}-iC^{-1}\bf{\mu})^TC (\bf{k}-iC^{-1}\bf{\mu}) + \mu^T C^{-1}\bf{\mu}
\end{gather*}
Therefore, the integral becomes 
\begin{gather*}
    \int d^nk \exp{\left[-i\bf{k}\cdot\bf{\mu}-\frac{1}{2}\bf{k}^TC\bf{k}\right]} = \int d^nk \exp{\left[-\frac{1}{2}\left(\bf{k}^TC\bf{k} + 2 i\bf{k}\cdot\bf{\mu} \right)\right]} =\\ 
    = \exp{\left[-\frac{1}{2}\left(\mu^T C^{-1}\mu\right) \right]}\int d^n k \exp{\left[-\frac{1}{2}(\bf{k}-iC^{-1}\bf{\mu})^TC (\bf{k}-iC^{-1}\bf{\mu})\right]}
\end{gather*}
Now inside the integral there is clearly a Gaussian with mean $iC^{-1}\mu$. So the integral
is equal to $(2\pi)^{n/2}(\det{C})^{-\frac{1}{2}}$.
\begin{gather*}
    \int d^nk \exp{\left[-i\bf{k}\cdot\bf{\mu}-\frac{1}{2}\bf{k}^TC\bf{k}\right]} = \exp{\left[-\frac{1}{2}\mu^T C^{-1}\mu \right]} (2\pi)^{n/2}(\det{C})^{-\frac{1}{2}}
\end{gather*}
which is simply a constant quantity. We have thus proved that the characteristic
function is an unnormalised Gaussian function.



\end{document}
