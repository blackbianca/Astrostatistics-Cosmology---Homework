\documentclass[a4paper,11pt,fleqn]{article}
\usepackage[utf8]{inputenc}
%\usepackage[italian]{babel}
%\usepackage{lipsum}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{multirow}
\usepackage{hyphenat}
\usepackage{sectsty}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{subfigure}
%\usepackage{color}
\usepackage{hyperref}


\title{Homework 3}
\author{Alessandro Bianchetti}

\begin{document}
\maketitle

\section{Exercise 8}
We want to show which is the most general form of Least Squares $\chi^{(2)}$
estimate: let's define the quantities in the game first. We assume we are 
working at a experiment that yields a data vector $\vec{d}=(d_1 \dotsc d_N)$: 
the average of the data is given by 
\begin{equation}
    \langle \vec{d} \rangle = \sum_{i=1}^M A_i \vec{t}_i = \vec{A}^TT    
\end{equation}
where $\vec{t}_i$ are a set of vectors whose linear combination is an 
estimate for the dataset, with coefficients $A_i$. $T$ is the MxN matrix 
grouping all the template vectors, while $\vec{A}$ is a M size vector. 
At this point, we assume data follows a Gaussian distribution with fixed 
covariance $C$, so we're enabled to build a $\chi^{(2)}$-like variable.
\begin{equation}
    \chi^{(2)}=(\vec{d}-\vec{A}^TT)^T C^{-1}(\vec{d}-\vec{A}^TT)
\end{equation} 
Our task is to minimize this quantity with respect to $\vec{A}$ in order to 
find the best estimates for the linear coefficients. We must thus perform 
a simple derivative $\frac{\partial \chi^{(2)}}{\partial A_l}$, but before 
starting with all the compulations we define a support variable 
$\vec{u} = \sum_jA_j\vec{t}_j$ or $u_i = \sum_j A_j T_{ij}$. If we 
introudce this new variable, then 
\begin{equation}
    \label{eqn:der}
    \frac{\partial \chi^{(2)}}{\partial A_l} = \sum_i \frac{\partial \chi^{(2)}}{\partial u_i}\frac{\partial u_i}{\partial A_l}
\end{equation}
The second derivative factor is pretty easy
\begin{equation*}
    \frac{\partial u_i}{\partial A_l} = \frac{\partial }{\partial A_l} \sum_j A_j T_{ij} = T_{il} 
\end{equation*}
while for the other one we have 
\begin{gather*}
    \frac{\partial \chi^{(2)}}{\partial u_i} = \frac{\partial}{\partial u_i}\left[\sum_{mn}(d_m-u_m)C^{-1}_{mn}(d_n-u_n)\right] = \\ 
    = \sum_{mn}\delta_{mi}C^{-1}_{mn}(d_n-u_n) + \sum_{mn}(d_m-u_m)C^{-1}_{mn}\delta_{ni} = \\ 
    = \sum_{n}C^{-1}_{in}(d_n-u_n) + \sum_{m}(d_m-u_m)C^{-1}_{mi} = 2\sum_n C^{-1}_{in}(d_n-u_n) 
\end{gather*}
where we exploited the symmetry of C. So overall, according to \ref{eqn:der}
\begin{equation*}
    \frac{\partial \chi^{(2)}}{\partial A_l} = -2 \sum_{in}T_{il}C^{-1}_{in}(d_n-u_n)
\end{equation*}
Now it's a matter of setting the latter equation to 0, so we can forget about 
the $-2$ factor before the sum: it's also the right 
moment to explicit the expression of $u_n$, which was hidding $\vec{A}$
until now.
\begin{gather*}
    \sum_{in}T_{il}C^{-1}_{in}(d_n-\sum_j A_j T_{jn}) = 0 \\
    \Rightarrow \sum_{in}T_{il}C^{-1}d_n = \sum_{ijn}T_{il}C^{-1}T_{jn}A_j
\end{gather*}
This means, in terms of matrices
\begin{equation}
    \label{eqn:GLS}
    TC^{-1}\vec{d}=TC^{-1}T\vec{A} \qquad \Rightarrow \qquad A = (TC^{-1}T)^{-1}TC^{-1}\vec{d}
\end{equation}
The latter is the most general form of a Least Squares (GLS) estimate, 
descending from a $\chi^{(2)}$ minimization, thus relating to a very specific 
case where we assume Gaussian behaviour for the dataset and a linear dependece 
of the mean on the parameters. In this case, both these requirements are 
fulfilled, therefore such minimization can be trusted.

\medskip

As a final conclusion, we should also check that the point we selected is actually a minimum: to 
do that we need to evaluate the second derivative 
\begin{equation*}
    \frac{\partial \chi^{(2)}}{\partial A_l \partial A_k} = \frac{\partial}{\partial A_k}\left[-2 \sum_{in}T_{il}C^{-1}_{in}(d_n-\sum_j A_j T_{jn})\right]\\ 
    = 2\sum_{in}T_{il}C^{-1}_{in}T_{kn}
\end{equation*}
and since C is positively defined, this quantity is always positive. The 
extreme point we found must be a minimum.

\section{Exercise 9}
We want to test the estimator we derived in the previous exercise in a
plausible experimental situation. Consider a dataset $\vec{d}$ of N 
different measurements, taken at different coordinates $x_i$. We want to 
fit the following linear model 
\begin{equation}
    \langle \vec{d}\rangle = \omega \vec{x} + \vec{b}
\end{equation}
where we assume Gaussian, uncorrelated noise. We want to find the MAP 
estimates for the parameters $\omega$ and $b$, and then their covariance. 
First, let's identify the matrices we will use as ingredients. $C^{-1}$ is 
the inverse of the covariance of the data, which we assume to be diagonal 
(uncorrelated signal). Being $C$ diagonal, $C^{-1}$ is diagonal too and the 
entries are given simply by the reciprocal of the variance of each 
measurement $(\sigma_i^2)^{-1}$. Then, $T$ is the matrix 
gathering the template vectors, which in our case are just two: $\vec{x}$ and 
$(1\dotsc1)$. So it's a 2xN matrix (M=2), where the the two vectors occupy 
the rows of the matrix. The coefficients $\omega$ and $b$ respectively assigned to the 
template vectors are the components of the vector $A$, gathering the 
parameters we want to estimate. 
We start from \ref{eqn:GLS} and apply to our case. First, let's work on 
$(TC^{-1}T)$.
\begin{gather*}
    (TC^{-1}T) = 
    \begin{pmatrix}
        -\vec{x}- \\
        1 \dotsc 1 \\
    \end{pmatrix}
    \begin{pmatrix}
        (\sigma_1^2)^{-1} &  & 0\\
        & \ddots & \\
        0 & & (\sigma_N^2)^{-1} \\
    \end{pmatrix}
    \begin{pmatrix}
        | & 1 \\
        \vec{x} & \vdots \\
        | & 1 \\
    \end{pmatrix}
    = \\ = 
    \begin{pmatrix}
        \sum_i \frac{x_i^2}{\sigma_i^2} & \sum_i \frac{x_i}{\sigma_i^2} \\
        \sum_i \frac{x_i}{\sigma_i^2} & \sum_i \frac{1}{\sigma_i^2}
    \end{pmatrix}
\end{gather*}
We need to invert the latter: since it is a 2x2 matrix, we can use the 
quick formula 
\begin{equation}
    M^{-1} = 
    \begin{pmatrix}
        a & b \\
        c & d \\
    \end{pmatrix}^{-1}
    = \frac{1}{\det M}
    \begin{pmatrix}
        d & -b \\
        -c & a \\
    \end{pmatrix}
\end{equation}
Thus, we get that 
\begin{equation}
    \label{eqn:cov_par}
    (TC^{-1}T)^{-1} = \frac{1}{\Delta}
    \begin{pmatrix}
        \sum_i \frac{1}{\sigma_i^2} & -\sum_i \frac{x_i}{\sigma_i^2} \\
        -\sum_i \frac{x_i}{\sigma_i^2} & \sum_i \frac{x_i^2}{\sigma_i^2} \\
    \end{pmatrix}
\end{equation}
where 
\begin{equation}
    \Delta = \det(TC^{-1}T) = \left(\sum_i\frac{x_i^2}{\sigma_i^2}\right)\left(\sum_i\frac{1}{\sigma_i^2}\right)-\left(\sum_i\frac{x_i}{\sigma_i^2}\right)^2
\end{equation}
Then we have 
\begin{gather*}
    TC^{-1} = 
    \begin{pmatrix}
        -\vec{x}- \\
        1 \dotsc 1 \\
    \end{pmatrix}
    \begin{pmatrix}
        (\sigma_1^2)^{-1} &  & 0\\
        & \ddots & \\
        0 & & (\sigma_N^2)^{-1} \\
    \end{pmatrix}
    = 
    \begin{pmatrix}
        (\sigma_1^2)^{-1}x_1 & \dotsc & (\sigma_N^2)^{-1}x_N \\
        (\sigma_1^2)^{-1} & \dotsc  & (\sigma_N^2)^{-1} \\
    \end{pmatrix}
\end{gather*}
Finally, 
\begin{gather*}
    TC^{-1}\vec{d} = 
    \begin{pmatrix}
        (\sigma_1^2)^{-1}x_1 & \dotsc & (\sigma_N^2)^{-1}x_N \\
        (\sigma_1^2)^{-1} & \dotsc  & (\sigma_N^2)^{-1} \\
    \end{pmatrix} \vec{d} = 
    \begin{pmatrix}
        \sum_i \frac{x_i d_i}{\sigma_i^2} \\
        \sum_i \frac{d_i}{\sigma_i^2} \\
    \end{pmatrix}
\end{gather*}
And we now have all the elements to build the vector $\vec{A}$, yielding the 
desired parameters.
\begin{gather*}
    \vec{A} = (TC^{-1}T)^{-1}TC^{-1}\vec{d} = \frac{1}{\Delta}
    \begin{pmatrix}
        \sum_i \frac{1}{\sigma_i^2} & -\sum_i \frac{x_i}{\sigma_i^2} \\
        -\sum_i \frac{x_i}{\sigma_i^2} & \sum_i \frac{x_i^2}{\sigma_i^2} \\
    \end{pmatrix}
    \begin{pmatrix}
        \sum_i \frac{x_i d_i}{\sigma_i^2} \\
        \sum_i \frac{d_i}{\sigma_i^2} \\
    \end{pmatrix} = \\ = \frac{1}{\Delta}
    \begin{pmatrix}
        \left(\sum_i\frac{1}{\sigma_i^2}\right)\left(\sum_i\frac{x_i d_i}{\sigma_i^2}\right) - \left(\sum_i\frac{d_i}{\sigma_i^2}\right)\left(\sum_i\frac{x_i}{\sigma_i^2}\right) \\
        \left(\sum_i\frac{d_i}{\sigma_i^2}\right) \left(\sum_i\frac{x_i^2}{\sigma_i^2}\right) - \left(\sum_i\frac{x_i}{\sigma_i^2}\right)\left(\sum_i\frac{x_i d_i}{\sigma_i^2}\right)
    \end{pmatrix}
\end{gather*}
In other words, slope and offset of the weighted linear fit are given by 
\begin{equation}
    \begin{cases}
        \omega = \frac{1}{\Delta} \left[\left(\sum_i\frac{1}{\sigma_i^2}\right)\left(\sum_i\frac{x_i d_i}{\sigma_i^2}\right) - \left(\sum_i\frac{d_i}{\sigma_i^2}\right)\left(\sum_i\frac{x_i}{\sigma_i^2}\right)\right] \\
        b = \frac{1}{\Delta} \left[ \left(\sum_i\frac{d_i}{\sigma_i^2}\right) \left(\sum_i\frac{x_i^2}{\sigma_i^2}\right) - \left(\sum_i\frac{x_i}{\sigma_i^2}\right)\left(\sum_i\frac{x_i d_i}{\sigma_i^2}\right)\right] \\
    \end{cases}
\end{equation}
Lastly, we can compute the variance of the parameters, in order to quantify
the errorbars that we can assign to the parameter estimators. To do that, 
we need to explicitly compute $Cov(A) = Cov((TC^{-1}T)^{-1}TC^{-1}\vec{d})$.
Remember that variance is a non-linear function: we need to make use of 
the property 
\begin{equation}
    Cov(K\vec{y}) = KCov(\vec{y})K^T
\end{equation}
where K is any constant matrix. In our case, 
\begin{gather*}
    Cov(A) =  Cov((TC^{-1}T)^{-1}TC^{-1}\vec{d}) = \\
    = (TC^{-1}T)^{-1}TC^{-1}Cov(\vec{d})((TC^{-1}T)^{-1}TC^{-1})^T = \\
    = (TC^{-1}T)^{-1}TC^{-1} C C^{-1}T(TC^{-1}T)^{-1} = \\ 
    = (TC^{-1}T)^{-1} (TC^{-1}T) (TC^{-1}T)^{-1} = (TC^{-1}T)^{-1}
\end{gather*}
where we used the fact that $Cov(\vec{d})=C$ and $C^{-1}C=\mathbf{1}$.
Therefore, the covariance matrix of slope and offset is given by 
\ref{eqn:cov_par} and we can write 
\begin{equation}
    \begin{cases}
        \sigma_{\omega} = \frac{1}{\Delta}\sum_i \frac{1}{\sigma_i^2} \\ 
        \sigma_b = \frac{1}{\Delta}\sum_i \frac{x_i^2}{\sigma_i^2} \\
    \end{cases}
\end{equation}
and the two parameters are also correlated, having the matrix non-zero 
off-diagonal terms.





\section{Exercise 10}
In this exercise we consider a n-dimensional dataset where each measurement 
$d_i$ is taken at the coordinate $\vec{x}_i$. We model the average with 
a linear combination of coordinates, properly weighted by coefficients:
\begin{equation*}
    \langle d_i \rangle = \beta_0 + \sum_{j=1}^p \beta_j x_{ij}
\end{equation*}
We also assume a zero-average, uncorrelated, Gaussian noise $\epsilon_i \in \mathcal{N}(0,\sigma^2)$.
If we assumed uniform priors on the coefficients, and since the above 
equation is linear in the parameters, we know that MAP reduces to least squares fitting. 
It's a matter of minimizing the following quantity 
\begin{equation}
    RSS = \sum_{i=1}^n (d_i - \beta_0 -\sum_j \beta_j x_{ij})^2
\end{equation}
which actually descends from the intention of maximizing a likelihood of 
the form $e^{-\frac{1}{2\sigma^2}RSS}$.
What happens if we decide to tackle the problem in a Bayesian sense and 
assume a non-uniform prior? Let's see a couple of cases. Remember that 
the posterior proportial to the likelihood times the prior, regardless 
of the normalizing coefficients and of the probability distribution of 
the data.
\begin{equation}
    \label{eqn:post}
    p(\beta_i|d_i) \propto p(d_i|\beta_i)p(\beta_i)
\end{equation}
In any case, we will assume all the parameters $\beta_i$ are \textit{iid} 
(independent identically distributed), meaning that they're independent 
(thus having diagonal covariance) but they follow the exact same 
distribution .



\subsection{Exponential prior}
We first try out an exponential distribution, properly normalised.
\begin{equation}
    p(\beta_i) = \frac{1}{2b}e^{-\frac{\beta_i}{b}}
\end{equation}
According to \ref{eqn:post}, the posterior can be quantified as 
\begin{gather*}
    p(\beta |\vec{d}) \propto e^{-\frac{1}{2\sigma^2}\sum_i (d_i-\beta_0-\sum_j \beta_j x_{ij})^2}e^{-\sum_i \frac{|\beta_i|}{b}} \\
    p(\beta |\vec{d}) \propto e^{-\frac{1}{2\sigma^2}\sum_i (d_i-\beta_0-\sum_j \beta_j x_{ij})^2-\sum_i \frac{|\beta_i|}{b}} \\
    p(\beta |\vec{d}) \propto e^{-\frac{1}{2\sigma^2}RSS-\sum_i \frac{|\beta_i|}{b}}
\end{gather*}
Maximizing this posterior is equivalent to minimizing the exponent changed of 
sign, thus the best estimates for the parameters $\beta_i$ are given by 
\begin{equation}
    \hat{\vec{\beta}} = \min{\left\{RSS + \frac{2\sigma^2}{b}\sum_i|\beta_i|\right\}}
\end{equation}

\subsection{Gaussian prior}
We can alternatively assume the candidate distribution for $\beta_i$ is 
Gaussian with variance $c$ and zero average.
\begin{equation}
    p(\beta_i) \propto e^{-\frac{1}{2c} \beta_i^2}
\end{equation}
This time
\begin{gather*}
    p(\beta |\vec{d}) \propto e^{-\frac{1}{2\sigma^2}\sum_i (d_i-\beta_0-\sum_j \beta_j x_{ij})^2-\frac{1}{2c}\sum_i \beta_i^2} \\
    p(\beta |\vec{d}) \propto e^{-\frac{1}{2\sigma^2} RSS -\frac{1}{2c}\sum_i \beta_i^2}
\end{gather*}
Therefore 
\begin{equation}
    \hat{\vec{\beta}} = \min{\left\{RSS + \frac{\sigma^2}{c}\sum_j \beta_j^2\right\}}
\end{equation}



\section{Exercise 12}
Say we have a data time series $D=\{d_1=d(t_1)\dotsc d_n=d(t_n)\}$, where 
the data are evenly spread, meaning time intervals are regular: 
$t_{i+1}-t_i = \Delta$ $ \forall i$. Say the data are all independent and 
that they can be modeled by a time-dependent sinusoidal signal $f(t)$ and 
a gaussian, zero average noise with fixed variance ( $n_i \in \mathcal{N}(0,\sigma^2)$).
\begin{equation}
    d_i = f(t_i)+n_i = B_1\cos(\omega t_i) + B_2 \sin(\omega t_i) + n_i
\end{equation}
Say our goal is to infer the unknown parameters $\omega, B_1, B_2$ and 
assume we work in the large frequency limit.

\subsection{Likelihood}
Let's start by writing a suitable form for the likelihood 
$\mathcal{L}(D|\omega, B_1, B_2)$. We assume a normalised Gaussian bell 
and write 
\begin{equation}
    \mathcal{L}(D|\omega, B_1, B_2) = (2\pi\sigma^2)^{-N/2}e^{-\frac{1}{2\sigma^2}\sum_i(d_i-B_1 \cos(\omega t_i)-B_2 \sin(\omega t_i))^2}
\end{equation}
We now define $Q = \sum_i(d_i-B_1 \cos(\omega t_i)-B_2 \sin(\omega t_i))^2$ and we show 
that this term can be explicited and simplified a bit. Also, in order to 
shrink notation, we set $c_i = \cos(\omega t_i)$ and $s_i = \sin(\omega t_i)$.
Now, expand Q by opening the square:
\begin{gather*}
    Q = \sum_i (d_i^2 + B_1^2 c_i^2 + B_2^2 s_i^2 -2B_1d_ic_i -2B_2d_is_i +2B_1B_2c_is_i) \\
    = N\bar{d^2}+B_1^2\sum_ic_i^2+B_2^2\sum_is_i^2 -2[B_1\sum_id_ic_i +B_2\sum_id_is_i]+2B_1B_2\sum_ic_is_i
\end{gather*}
It can be proven that in the high frequency limit the last term 
can be neglected: let's show this immediately. First, we recall 
$\sin(2\alpha) = 2\sin(\alpha)\cos(\alpha)$.
\begin{gather*}
    2\sum_is_ic_i = \sum_i\sin(2\omega t_i)
\end{gather*}
Then we transit in the continuous time regime, substituting the discrete sum 
with an integral
\begin{gather*}
    \sum_i\sin(2\omega t_i) = \frac{1}{t_N-t_1}\int_{t_1}^{t_N}\sin(2\omega t) dt = \frac{1}{N\Delta} \frac{1}{2\omega}\left[-\cos(2\omega t)\right]_{t_1}^{t_N}
\end{gather*} 
We must immediately notice that $\sum_ic_is_i\propto (\omega\Delta)^{-1}$, 
and since we're working with high frequencies $\omega\Delta >> 1$, allowing 
to neglect the term under exam.

Now provide the following definitions: $\sum_ic_i^2 = c$, $\sum_is_i^2 = s$, 
$\sum_i d_i c_i = R(\omega)$, $\sum_i d_i s_i = I(\omega)$. 
Thanks to these new definitions and neglecting the last sum, we get 
\begin{equation}
    Q = N\bar{d^2}+B_1^2c+B_2^2s -2[B_1R(\omega) +B_2I(\omega)]
\end{equation}


\subsection{$\omega$-independent terms}
See that we did not specify the $\omega$ dependance on $c$ and $s$, as 
in first approximation they are independent on frequency. To show this, 
we make use of the continuous approximation again, together with the high 
frquency limit. 
\begin{gather*}
    s = \sum_i \sin^2(\omega t_i)\approx \frac{1}{N\Delta}\int_{t_1}^{t_N} \sin^2(\omega t)dt =\\
    = \frac{1}{2N\Delta} \int_{t_1}^{t_N}(1-\cos(2\omega t))dt = \frac{1}{2N}-\frac{1}{2N \Delta \cdot 2\omega}[\sin(2\omega t)]_{t_1}^{t_N}\approx \frac{1}{2N}
\end{gather*}
\begin{gather*}
    c = \sum_i \cos^2(\omega t_i)\approx \frac{1}{N\Delta}\int_{t_1}^{t_N} \cos^2(\omega t)dt =\\
    = \frac{1}{2N\Delta} \int_{t_1}^{t_N}(\cos(2\omega t)+1)dt = \frac{1}{2N}+\frac{1}{2N \Delta \cdot 2\omega}[\sin(2\omega t)]_{t_1}^{t_N}\approx \frac{1}{2N}
\end{gather*}
Therefore we can treat $s$ and $c$ as $\omega$-independent terms.

\subsection{MAP for a uniform prior}
We now have an expression for the likelihood, and if we assume uniform priors, 
than the posterior is actually proportional to the likelihood: Bayesian 
treatment and frequentist approach kind of overlap. 
\begin{equation}
    p(\omega, B_1, B_2|D) \propto \mathcal{L}(D|\omega, B_1, B_2) \propto e^{-\frac{1}{2\sigma^2}Q}
\end{equation}
Since our target parameter is $\omega$, we treat $B_1$ and $B_2$ as nuisance 
parameters. In order to get rid of them, we want to marginalize the 
posterior over their domains. Since we're talking about amplitudes, we can 
reasonably take the interval $[0,\infty]$ as a domain. 
\begin{equation}
    p(\omega|D) = \int_0^{\infty} \int_0^{\infty}  p(\omega, B_1, B_2|D) dB_1dB_2
\end{equation}
Note that, within $Q$, the term $N\bar{d^2}$ is independent on these 
amplitudes, therefore can be factored out of the integral as a constant.
At this point, let's explicit the interesting dependences of the exponent 
and solve it like a Gaussian integral.
\begin{gather*}
    p(\omega|D) \propto \int_0^{\infty} \int_0^{\infty}   e^{-\frac{1}{2\sigma^2}[B_1^2c + B_2^2 s -2(B_1R(\omega)+B_2I(\omega))]}dB_1dB_2 = \\
    = \int_0^{\infty}e^{-\frac{1}{2\sigma^2}(B_1^2c-2B_1R(\omega))}dB_1 \int_0^{\infty}e^{-\frac{1}{2\sigma^2}(B_2^2s-2B_2I(\omega))}dB_2
\end{gather*}
We now proceed with a square completion, adding and subtracting the term 
$\frac{1}{2\sigma^2}\frac{R^2}{c}$ in the exponent of the first integral and $\frac{1}{2\sigma^2}\frac{I^2}{s}$ in the second one.
\begin{gather*}
    p(\omega|D) \propto \left[\int_0^{\infty}e^{-\frac{1}{2\sigma^2}(B_1^2c-2B_1R(\omega)+\frac{R^2}{c})}dB_1 \int_0^{\infty}e^{-\frac{1}{2\sigma^2}(B_2^2s-2B_2I(\omega))+\frac{I^2}{s})}dB_2\right] e^{\frac{1}{2\sigma^2}\left(\frac{R^2}{c}+\frac{I^2}{s}\right)} =\\
    = \left[\int_0^{\infty} e^{-\frac{1}{2}\left(\frac{B_1\sqrt{c}-R/\sqrt{c}}{\sigma}\right)^2 }dB_1 \int_0^{\infty} e^{-\frac{1}{2}\left(\frac{B_2\sqrt{s}-I/\sqrt{s}}{\sigma}\right)^2 }dB_2\right] e^{\frac{1}{2\sigma^2}\left(\frac{R^2}{c}+\frac{I^2}{s}\right)} = \\
    = \frac{1}{2}\sqrt{\frac{2\pi\sigma^2}{c}} \cdot \frac{1}{2}\sqrt{\frac{2\pi\sigma^2}{s}}e^{\frac{1}{2\sigma^2}\left(\frac{R^2}{c}+\frac{I^2}{s}\right)}
\end{gather*}
So overall
\begin{equation}
    p(\omega|D) \propto \frac{1}{\sqrt{cs}} e^{\frac{1}{2\sigma^2}\left(\frac{R^2}{c}+\frac{I^2}{s}\right)}
\end{equation}

\subsection{Maximum A Posteriori estimate}
We want to prove that the $\omega$ value that maximizes the obtained 
posterior is the same that maximizes the Fourier transform of the power 
spectrum of the signal, called \textit{periodogram} $C(\omega)$.
\begin{equation}
    C(\omega) = \frac{2}{N} \Big| \sum_{k=1}^N d_k e^{-i\omega t_k}\Big|^2
\end{equation}
Let's first try to understand which is the condition to fulfill in order 
to maximize $p(\omega|D)$. Since it has an exponential behaviour, it is 
required we maximize its exponent, which is $R^2/c + I^2/s$. However,
we proved that $c\approx s$ in the high frequency limit, so we actually 
need to maximize just $R^2+I^2$.
\begin{equation}
    \label{eqn:max}
    \hat{\omega} = argmax\left\{p(\omega|D)\right\} = argmax\left\{R^2(\omega)+I^2(\omega)\right\}
\end{equation}
Let's check whether this condition is the same for the periodogram. 
\begin{gather*}
    C(\omega) = \frac{2}{N} \Big| \sum_{k=1}^N d_k e^{-i\omega t_k}\Big|^2 = \frac{2}{N} \Big| \sum_{k=1}^N d_k \cos(\omega t_k) -i \sum_{k=1}^N d_k \sin(\omega t_k) \Big|^2 = \\
    = \frac{2}{N} \left[\left(\sum_k d_k \cos(\omega t_k)\right)^2 + \left(\sum_k d_k \sin(\omega t_k)\right)^2\right] = \frac{2}{N}\left[R^2(\omega)+I^2(\omega)\right] 
\end{gather*}
So clearly, the maximization condition on $C(\omega)$ is the same we identified 
for the posterior distribution \ref{eqn:max}.

\subsection{Least Squares Fitting}
The case we have just studied is a nice example of how Least Squares fitting 
is not always a suitable approach for the problem. We have studied that LS 
only works when we can count on Gaussian signal and when we can assume 
that the parameters dependence is only encoded in the mean and is furthermore 
linear.
There actually is a condition under which these requirements could be fulfilled 
in this problem too, that is when the sinusoidal model $f(t)$ can be 
approximated as linear. Note that if $\omega t << 1$, like in a low-frequency 
case, then $f(t) \approx B_1 + B_2\omega t$ and we have a proper environment 
for a LS fitting.


\end{document}
